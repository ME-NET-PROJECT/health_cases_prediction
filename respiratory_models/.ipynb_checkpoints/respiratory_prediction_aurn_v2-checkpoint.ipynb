{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57a8a8b-ca84-41ec-9f2b-dedd13593aab",
   "metadata": {},
   "source": [
    "# Respiratory Cases Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9bff0-389c-413b-ab34-2b11244c84ec",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ca982-05ba-4372-b284-80849addcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, calendar \n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense,\n",
    "    Bidirectional, GRU, Add\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,  mean_squared_log_error\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74ec48-5216-4cfe-a9a0-34849fa92c47",
   "metadata": {},
   "source": [
    "## 2. Parameter Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b90df-3c00-43c4-9154-8e689d39113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = f\"datasets/UK/\"\n",
    "dataset = \"respiratory_climate_summary.csv\"\n",
    "model_dataset = \"aurn_cams_respiratory\"\n",
    "models_folder = Path(dataset_folder) / Path(model_dataset) /Path(\"models\")\n",
    "stations_file = Path(dataset_folder) / \"monitoring_stations.csv\"\n",
    "sequence_length=5\n",
    "\n",
    "req_cols = [\n",
    "    'Time', 'SiteNumber', 'Longitude', 'Latitude', \n",
    "    'aurn_go3_max', 'aurn_no2', 'ch4_c', 'uvbed', 'uvbedcs', 't', 'ws', 'cases'\n",
    "]\n",
    "\n",
    "engineer_cols=['aurn_go3_max', 'aurn_no2', 'ch4_c', 'uvbed', 'uvbedcs', 't', 'ws']\n",
    "\n",
    "exclude_cols = ['Time', 'SiteNumber', 'Longitude', 'Latitude']\n",
    "\n",
    "y_columns=['cases']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951fdd8-4d1e-49f6-a812-6c0cc08fc7d6",
   "metadata": {},
   "source": [
    "## 3. A Function to Get the AURN site Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e14c139e-7c32-46d9-afbc-5ed2a557d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_info(site_number, stations_file, site_column=\"SiteNumber\"):\n",
    "    try:\n",
    "        # Load the CSV\n",
    "        stations_df = pd.read_csv(stations_file)\n",
    "        \n",
    "        # Ensure SiteNumber is treated as a string\n",
    "        stations_df[site_column] = stations_df[site_column].astype(str)\n",
    "        site_number = str(site_number)\n",
    "\n",
    "        # Find the matching row\n",
    "        site_info = stations_df[stations_df[site_column] == site_number]\n",
    "        \n",
    "        if not site_info.empty:\n",
    "            site_info = site_info.iloc[0]  # Get the first match\n",
    "            return site_info[\"SiteName\"], site_info[\"Longitude\"], site_info[\"Latitude\"]\n",
    "        else:\n",
    "            return None, None, None  # Return None if not found\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {stations_file} not found.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading site info: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4fd36-38d6-43e5-aeed-c7c72963d1cc",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e80c8302-95b7-4910-a51b-fbc5f419bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(dataset_folder)/Path(dataset), parse_dates=['Time'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d916314-96ad-443c-aecd-7b7f639d3026",
   "metadata": {},
   "source": [
    "#### Compute Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e973460-4609-4ac8-adc5-5539fe959c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ws'] = np.sqrt(df['u']**2 + df['v']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a20616-b8d4-4027-b7aa-4087e1d53ba6",
   "metadata": {},
   "source": [
    "### Group Health Cases  \n",
    "We do not want to group by primary impression codes, as this leads to severe imbalance. Therefore, we will predict all cases together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "288b31ab-6c6b-4ccd-a6e4-b67f595cb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Time', 'SiteNumber', 'Longitude', 'Latitude'\n",
    "df = df.groupby(['Time', 'SiteNumber', 'Longitude', 'Latitude'], as_index=False).agg({\n",
    "    'aurn_go3_max': 'mean',\n",
    "    'aurn_no2': 'mean',\n",
    "    'ch4_c': 'mean',\n",
    "    'uvbed': 'mean',\n",
    "    'uvbedcs': 'mean',\n",
    "    't': 'mean',\n",
    "    'ws': 'mean',\n",
    "    'Primary_Impression_Count': 'sum'  # Summing impression counts\n",
    "}).rename(columns={'Primary_Impression_Count': 'cases'})  # Rename column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a4249-bf73-4800-a18d-616aea5dc928",
   "metadata": {},
   "source": [
    "#### Include Only Required Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3b1e313-ec58-4362-a158-6816bdbe7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[req_cols].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ed661-1441-4bba-b3fa-f97af5030a07",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89425e9-a3d5-4ea1-abc0-a4083e309c65",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedb60b-c7a4-49d5-ba5d-5d105c186f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_features(\n",
    "    df, degree=2, interaction_only=False, include_bias=False, include_columns=None\n",
    "):\n",
    "    # Handle columns to include\n",
    "    include_columns = include_columns or df.columns.tolist()  # Default to all columns if None\n",
    "    included_df = df[include_columns]  # DataFrame with only included columns\n",
    "    excluded_df = df.drop(columns=include_columns, errors='ignore')  # Columns not used for polynomial features\n",
    "\n",
    "    # Generate polynomial features on the included columns\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=include_bias)\n",
    "    poly_features = poly.fit_transform(included_df.values)\n",
    "    feature_names = poly.get_feature_names_out(included_df.columns)\n",
    "    \n",
    "    # Create DataFrame for polynomial features\n",
    "    poly_df = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n",
    "    \n",
    "    # Concatenate excluded columns back with the polynomial features\n",
    "    final_df = pd.concat([poly_df, excluded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "df = generate_polynomial_features(\n",
    "    df, \n",
    "    degree=2, \n",
    "    interaction_only=False, \n",
    "    include_bias=False, \n",
    "    include_columns=engineer_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87667ef1-95e5-4a2e-858c-8a31fd0fbd98",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ff35d4d-b084-4568-a21a-1424feded529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_in_year(year):\n",
    "    return 366 if calendar.isleap(year) else 365\n",
    "        \n",
    "def temporal_cyclical_features(df, time_col='Time'):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert the time column to datetime if not already\n",
    "    if not np.issubdtype(df[time_col].dtype, np.datetime64):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Extract temporal features\n",
    "    df['Year'] = df[time_col].dt.year\n",
    "    df['DayOfWeek'] = df[time_col].dt.weekday                   # Day of the week (0-6)\n",
    "    df['DayOfYear'] = df[time_col].dt.dayofyear                 # Day of the year (1-365/366)\n",
    "    df['Month'] = df[time_col].dt.month                         # Month of the year (1-12)\n",
    "    df['IsWeekend'] = df[time_col].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)  # Weekday vs Weekend\n",
    "    # Extract ISO Week Number (WeekOfYear)\n",
    "    df['WeekOfYear'] = df[time_col].dt.isocalendar().week\n",
    "    # Handle Week 53 cases by mapping them to 52\n",
    "    df['WeekOfYear'] = df['WeekOfYear'].apply(lambda x: x if x <= 52 else 52)\n",
    "\n",
    "    # Compute the Lunar Phase (0 = New Moon, 14-15 = Full Moon, 29 = Next New Moon)\n",
    "    df['LunarDay'] = (df[time_col] - pd.Timestamp(\"2000-01-06\")).dt.days % 29.53  # Reference New Moon date\n",
    "\n",
    "    # Determine the season (0: Winter, 1: Spring, 2: Summer, 3: Fall)\n",
    "    df['Season'] = df[time_col].apply(lambda x: \n",
    "        0 if x.month in [12, 1, 2] else \n",
    "        1 if x.month in [3, 4, 5] else \n",
    "        2 if x.month in [6, 7, 8] else \n",
    "        3\n",
    "    )\n",
    "\n",
    "    df['DaysInYear'] = df['Year'].apply(get_days_in_year)\n",
    "\n",
    "    # Cyclical encoding for the day of the year\n",
    "    df['DayOfYearSin'] = np.sin(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
    "    df['DayOfYearCos'] = np.cos(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
    "\n",
    "    df['DayOfWeekSin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    df['DayOfWeekCos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "\n",
    "    df['MonthSin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['MonthCos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    df['WeekOfYearSin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52)\n",
    "    df['WeekOfYearCos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52)\n",
    "\n",
    "    # Cyclical Encoding of the Lunar Cycle\n",
    "    df['LunarSin'] = np.sin(2 * np.pi * df['LunarDay'] / 29.53)\n",
    "    df['LunarCos'] = np.cos(2 * np.pi * df['LunarDay'] / 29.53)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = temporal_cyclical_features(df, time_col='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2dd192-a226-432d-bb80-f549e9aee191",
   "metadata": {},
   "source": [
    "### Min/Max Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b72e4c-abd9-44de-91c7-c0e3bb9446d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_features(df, y_columns, exclude_x_columns=None):\n",
    "    # Separate features and target\n",
    "    if exclude_x_columns:\n",
    "        y_columns = y_columns + exclude_x_columns\n",
    "        \n",
    "    features = df.drop(columns=y_columns)\n",
    "    target = df[y_columns]\n",
    "\n",
    "    # Apply Min-Max Scaling to features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Convert back to DataFrame and retain original column names\n",
    "    scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns, index=df.index)\n",
    "    \n",
    "    # Combine scaled features with the target column\n",
    "    result_df = pd.concat([scaled_features_df, target], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "df = min_max_scale_features(df, y_columns=y_columns, exclude_x_columns=exclude_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a1644-ce15-41f5-a9f6-1ca9a98f9255",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8c00bcd-0540-49fe-9dde-909ab3d1e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-01-01'\n",
    "train_end = '2023-12-31'\n",
    "\n",
    "# The training set split (includes train and validation)\n",
    "df_train = df[(df['Time'] >= train_start) & (df['Time'] <= train_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "476b03ec-e568-48b7-90de-9b89dc713b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = '2024-01-01'\n",
    "test_end = '2024-08-31'\n",
    "\n",
    "# The training set split (includes train and validation)\n",
    "df_test = df[(df['Time'] >= test_start) & (df['Time'] <= test_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "779140c7-3c3b-4ce0-b905-67545dd15670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7232, 59), (999, 59))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48419f20-52c0-43f7-885a-672b75106be7",
   "metadata": {},
   "source": [
    "### Create Training Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4213fa27-f49a-4607-8ea3-8db479720774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sites: 100%|████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.64it/s]\n",
      "Processing Sites: 100%|████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00, 11.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def create_sequences(df, seq_length, y_columns):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    metadata = []  # List to store Time and SiteNumber information\n",
    "    \n",
    "    for i in range(len(df) - seq_length):\n",
    "        seq = df.iloc[i:i + seq_length]\n",
    "        # Check if the sequence has consecutive days\n",
    "        time_diff = (seq['Time'].diff().dropna() == pd.Timedelta(days=1)).all()\n",
    "        \n",
    "        if not time_diff:  # Skip sequences with non-consecutive days\n",
    "            continue\n",
    "\n",
    "        # Include lagged y_column(s) as features within the sequence\n",
    "        lagged_features = seq[y_columns].values  # Extract the lagged `y_columns`\n",
    "        other_features = seq[df.columns.drop(['Time', 'SiteNumber', *y_columns])].values\n",
    "        sequence_features = np.hstack((other_features, lagged_features))  # Concatenate features\n",
    "\n",
    "        \n",
    "        # Append the valid sequence and target\n",
    "        sequences.append(seq[df.columns.drop(['Time', 'SiteNumber'])].values)\n",
    "        targets.append(df.iloc[i + seq_length][y_columns])\n",
    "        \n",
    "        # Store metadata for sequence\n",
    "        metadata.append({\n",
    "            \"Start_Time\": seq.iloc[0]['Time'],  # Start time of the sequence\n",
    "            \"End_Time\": seq.iloc[-1]['Time'],  # End time of the sequence\n",
    "            \"SiteNumber\": seq.iloc[0]['SiteNumber'],  # Site number for the sequence\n",
    "        })\n",
    "    \n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), metadata_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_sequences_by_site(df, site_column, seq_length, y_columns):\n",
    "    all_sequences = []\n",
    "    all_targets = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    # Iterate over each unique site\n",
    "    for site, group in tqdm(df.groupby(site_column), desc=\"Processing Sites\"):\n",
    "        # Sort the group by time\n",
    "        group = group.sort_values(by='Time')\n",
    "\n",
    "        # Create sequences for this group\n",
    "        site_sequences, site_targets, site_metadata = create_sequences(group, seq_length, y_columns)\n",
    "\n",
    "        # Append sequences and targets\n",
    "        if site_sequences.size > 0:  # Only add non-empty sequences\n",
    "            all_sequences.append(site_sequences)\n",
    "            all_targets.append(site_targets)\n",
    "            all_metadatas.append(site_metadata)\n",
    "\n",
    "    # Concatenate sequences and targets across all sites\n",
    "    if all_sequences:\n",
    "        all_sequences = np.concatenate(all_sequences, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        all_metadatas = np.concatenate(all_metadatas, axis=0)\n",
    "    else:\n",
    "        all_sequences = np.array([])\n",
    "        all_targets = np.array([])\n",
    "        all_metadatas = np.array([])\n",
    "\n",
    "    # Return sequences and targets\n",
    "    return all_sequences, all_targets, all_metadatas\n",
    "\n",
    "\n",
    "X_train, y_train, X_train_meta = generate_sequences_by_site(df_train, 'SiteNumber', sequence_length, y_columns)\n",
    "X_test, y_test, X_test_meta = generate_sequences_by_site(df_test, 'SiteNumber', sequence_length, y_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c448638-eeda-41b7-b0af-d33df032e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data types are float32\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1951a9d7-7c92-4400-bb43-11427ae02ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4658, 5, 57), (4658, 1), (4658, 3), (738, 5, 57), (738, 1), (738, 3))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_train_meta.shape,  X_test.shape, y_test.shape, X_test_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa675aa-65fc-4c97-be85-9d1352797467",
   "metadata": {},
   "source": [
    "## 6. ML Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca9a4c-9e19-4e52-9691-38d220390386",
   "metadata": {},
   "source": [
    "### 1. LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45008f2c-bff8-4a1c-811e-b59a2720fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based model\n",
    "def lstm_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  LSTM(units=50, return_sequences=True)(input_layer)\n",
    "    x =  LSTM(units=50, return_sequences=False)(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65d7a7d-136e-40c2-a010-0d963b8467e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# model1 = bi_lstm_model(sequence_length, X_train.shape[2])\n",
    "# # Save and display the model architecture\n",
    "# plot_model(model1, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19c2cb-3d7e-4c7c-9bc4-019a9b6967a9",
   "metadata": {},
   "source": [
    "### 2. Bi-Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69a0497-f083-4b6b-906f-05390d719cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-directional LSTM-based model\n",
    "def bi_lstm_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  Bidirectional(LSTM(units=256, return_sequences=True))(input_layer)\n",
    "    \n",
    "    # Feed-forward layer on the sequence\n",
    "    x_ffn = Dense(units=512, activation='relu')(x)  # Apply Dense layer to sequence\n",
    "    x =  Bidirectional(LSTM(units=256, return_sequences=True))(x)\n",
    "    \n",
    "    # Residual connection: add bi-directional LSTM output and Dense output\n",
    "    x = Add()([x, x_ffn])\n",
    "    \n",
    "    # Final bi-directional LSTM layers for sequence compression\n",
    "    x =  Bidirectional(LSTM(units=128, return_sequences=True))(x)\n",
    "    x =  Bidirectional(LSTM(units=64, return_sequences=True))(x)\n",
    "    x =  Bidirectional(LSTM(units=32, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a598f-17fc-4986-8fe8-d666aa6849eb",
   "metadata": {},
   "source": [
    "### 3. Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c68c1621-f02e-457b-9021-e3117413c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "def gru_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  GRU(units=50, return_sequences=True)(input_layer)\n",
    "    x =  GRU(units=50, return_sequences=False)(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d29efe-9566-4808-816d-f9a604ca3c0a",
   "metadata": {},
   "source": [
    "### 4. Bi-Directional Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62de2a39-fa8a-430d-bbc7-7db3d86ff447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-directional GRU-based model\n",
    "def bi_gru_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  Bidirectional(GRU(units=256, return_sequences=True))(input_layer)\n",
    "    \n",
    "    # Feed-forward layer on the sequence\n",
    "    x_ffn = Dense(units=512, activation='relu')(x)  # Apply Dense layer to sequence\n",
    "    x =  Bidirectional(GRU(units=256, return_sequences=True))(x)\n",
    "    \n",
    "    # Residual connection: add bi-directional LSTM output and Dense output\n",
    "    x = Add()([x, x_ffn])\n",
    "    \n",
    "    # Final bi-directional LSTM layers for sequence compression\n",
    "    x =  Bidirectional(GRU(units=128, return_sequences=True))(x)\n",
    "    x =  Bidirectional(GRU(units=64, return_sequences=True))(x)\n",
    "    x =  Bidirectional(GRU(units=32, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5a260-f661-4f8f-aa35-92cf28804c53",
   "metadata": {},
   "source": [
    "### 4. Train and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26157678-1fe5-49d3-97fd-6ea4f7e6a3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm model...\n",
      "Epoch 1/200\n",
      "140/146 [===========================>..] - ETA: 0s - loss: 0.1359  \n",
      "Epoch 1: val_loss improved from inf to 0.07541, saving model to datasets\\UK\\aurn_cams\\models\\lstm.h5\n",
      "146/146 [==============================] - 4s 14ms/step - loss: 0.1340 - val_loss: 0.0754 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0637\n",
      "Epoch 2: val_loss improved from 0.07541 to 0.04110, saving model to datasets\\UK\\aurn_cams\\models\\lstm.h5\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 0.0636 - val_loss: 0.0411 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0482\n",
      "Epoch 3: val_loss improved from 0.04110 to 0.03758, saving model to datasets\\UK\\aurn_cams\\models\\lstm.h5\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0481 - val_loss: 0.0376 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.0452\n",
      "Epoch 4: val_loss did not improve from 0.03758\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0452 - val_loss: 0.0383 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0444\n",
      "Epoch 5: val_loss improved from 0.03758 to 0.03636, saving model to datasets\\UK\\aurn_cams\\models\\lstm.h5\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0444 - val_loss: 0.0364 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "140/146 [===========================>..] - ETA: 0s - loss: 0.0440\n",
      "Epoch 6: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0440 - val_loss: 0.0372 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.0435\n",
      "Epoch 7: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0436 - val_loss: 0.0387 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "141/146 [===========================>..] - ETA: 0s - loss: 0.0439\n",
      "Epoch 8: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0438 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0433\n",
      "Epoch 9: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0433 - val_loss: 0.0375 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0431\n",
      "Epoch 10: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0431 - val_loss: 0.0369 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0431\n",
      "Epoch 11: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0431 - val_loss: 0.0369 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "140/146 [===========================>..] - ETA: 0s - loss: 0.0428\n",
      "Epoch 12: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0428 - val_loss: 0.0382 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0424\n",
      "Epoch 13: val_loss did not improve from 0.03636\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 0.0424 - val_loss: 0.0369 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0424\n",
      "Epoch 14: val_loss improved from 0.03636 to 0.03622, saving model to datasets\\UK\\aurn_cams\\models\\lstm.h5\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 0.0425 - val_loss: 0.0362 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0427\n",
      "Epoch 15: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 0.0426 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "141/146 [===========================>..] - ETA: 0s - loss: 0.0428\n",
      "Epoch 16: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0427 - val_loss: 0.0375 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "141/146 [===========================>..] - ETA: 0s - loss: 0.0428\n",
      "Epoch 17: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 0.0428 - val_loss: 0.0371 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 18: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 0.0422 - val_loss: 0.0365 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0426\n",
      "Epoch 19: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0425 - val_loss: 0.0389 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 20: val_loss did not improve from 0.03622\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0422 - val_loss: 0.0369 - lr: 0.0010\n",
      "lstm model saved!\n",
      "Training bi_lstm model...\n",
      "Epoch 1/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1095   \n",
      "Epoch 1: val_loss improved from inf to 0.05050, saving model to datasets\\UK\\aurn_cams\\models\\bi_lstm.h5\n",
      "146/146 [==============================] - 14s 41ms/step - loss: 0.1091 - val_loss: 0.0505 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0497\n",
      "Epoch 2: val_loss improved from 0.05050 to 0.03779, saving model to datasets\\UK\\aurn_cams\\models\\bi_lstm.h5\n",
      "146/146 [==============================] - 3s 23ms/step - loss: 0.0497 - val_loss: 0.0378 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0461\n",
      "Epoch 3: val_loss improved from 0.03779 to 0.03625, saving model to datasets\\UK\\aurn_cams\\models\\bi_lstm.h5\n",
      "146/146 [==============================] - 4s 24ms/step - loss: 0.0461 - val_loss: 0.0363 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0450\n",
      "Epoch 4: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 22ms/step - loss: 0.0450 - val_loss: 0.0369 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0451\n",
      "Epoch 5: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 23ms/step - loss: 0.0452 - val_loss: 0.0387 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0445\n",
      "Epoch 6: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 21ms/step - loss: 0.0445 - val_loss: 0.0389 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0445\n",
      "Epoch 7: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0445 - val_loss: 0.0402 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0439\n",
      "Epoch 8: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 22ms/step - loss: 0.0438 - val_loss: 0.0374 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0442\n",
      "Epoch 9: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 21ms/step - loss: 0.0442 - val_loss: 0.0397 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0436\n",
      "Epoch 10: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0435 - val_loss: 0.0364 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0427\n",
      "Epoch 11: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 21ms/step - loss: 0.0428 - val_loss: 0.0399 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0434\n",
      "Epoch 12: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 3s 21ms/step - loss: 0.0433 - val_loss: 0.0382 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0430\n",
      "Epoch 13: val_loss did not improve from 0.03625\n",
      "146/146 [==============================] - 4s 24ms/step - loss: 0.0430 - val_loss: 0.0387 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0418\n",
      "Epoch 14: val_loss improved from 0.03625 to 0.03606, saving model to datasets\\UK\\aurn_cams\\models\\bi_lstm.h5\n",
      "146/146 [==============================] - 3s 22ms/step - loss: 0.0418 - val_loss: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Epoch 15: val_loss improved from 0.03606 to 0.03597, saving model to datasets\\UK\\aurn_cams\\models\\bi_lstm.h5\n",
      "146/146 [==============================] - 3s 22ms/step - loss: 0.0415 - val_loss: 0.0360 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0416\n",
      "Epoch 16: val_loss did not improve from 0.03597\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0416 - val_loss: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Epoch 17: val_loss did not improve from 0.03597\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0415 - val_loss: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Epoch 18: val_loss did not improve from 0.03597\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0414 - val_loss: 0.0362 - lr: 1.0000e-04\n",
      "bi_lstm model saved!\n",
      "Training gru model...\n",
      "Epoch 1/200\n",
      "139/146 [===========================>..] - ETA: 0s - loss: 0.1047  \n",
      "Epoch 1: val_loss improved from inf to 0.05589, saving model to datasets\\UK\\aurn_cams\\models\\gru.h5\n",
      "146/146 [==============================] - 3s 10ms/step - loss: 0.1031 - val_loss: 0.0559 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "141/146 [===========================>..] - ETA: 0s - loss: 0.0522\n",
      "Epoch 2: val_loss improved from 0.05589 to 0.03847, saving model to datasets\\UK\\aurn_cams\\models\\gru.h5\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0385 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 3: val_loss improved from 0.03847 to 0.03700, saving model to datasets\\UK\\aurn_cams\\models\\gru.h5\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0463 - val_loss: 0.0370 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0445\n",
      "Epoch 4: val_loss did not improve from 0.03700\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0401 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0444\n",
      "Epoch 5: val_loss improved from 0.03700 to 0.03667, saving model to datasets\\UK\\aurn_cams\\models\\gru.h5\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0441 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0440\n",
      "Epoch 6: val_loss did not improve from 0.03667\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0377 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0438\n",
      "Epoch 7: val_loss did not improve from 0.03667\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0383 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "139/146 [===========================>..] - ETA: 0s - loss: 0.0432\n",
      "Epoch 8: val_loss improved from 0.03667 to 0.03633, saving model to datasets\\UK\\aurn_cams\\models\\gru.h5\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0363 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0428\n",
      "Epoch 9: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0430\n",
      "Epoch 10: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0376 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0426\n",
      "Epoch 11: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0428\n",
      "Epoch 12: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0370 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0429\n",
      "Epoch 13: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0374 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "139/146 [===========================>..] - ETA: 0s - loss: 0.0424\n",
      "Epoch 14: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0393 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0423\n",
      "Epoch 15: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0378 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0425\n",
      "Epoch 16: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "138/146 [===========================>..] - ETA: 0s - loss: 0.0420\n",
      "Epoch 17: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "141/146 [===========================>..] - ETA: 0s - loss: 0.0420\n",
      "Epoch 18: val_loss did not improve from 0.03633\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0420 - val_loss: 0.0373 - lr: 0.0010\n",
      "gru model saved!\n",
      "Training bi_gru model...\n",
      "Epoch 1/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.1270   \n",
      "Epoch 1: val_loss improved from inf to 0.10674, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 13s 33ms/step - loss: 0.1268 - val_loss: 0.1067 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.1146\n",
      "Epoch 2: val_loss improved from 0.10674 to 0.09564, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 3s 18ms/step - loss: 0.1146 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0671\n",
      "Epoch 3: val_loss improved from 0.09564 to 0.04204, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0668 - val_loss: 0.0420 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0471\n",
      "Epoch 4: val_loss improved from 0.04204 to 0.03724, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0471 - val_loss: 0.0372 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0464\n",
      "Epoch 5: val_loss did not improve from 0.03724\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0464 - val_loss: 0.0376 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 6: val_loss did not improve from 0.03724\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0456 - val_loss: 0.0373 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0454\n",
      "Epoch 7: val_loss did not improve from 0.03724\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0453 - val_loss: 0.0402 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0449\n",
      "Epoch 8: val_loss did not improve from 0.03724\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0448 - val_loss: 0.0399 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0447\n",
      "Epoch 9: val_loss improved from 0.03724 to 0.03709, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0447 - val_loss: 0.0371 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0440\n",
      "Epoch 10: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0440 - val_loss: 0.0401 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0441\n",
      "Epoch 11: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 3s 17ms/step - loss: 0.0441 - val_loss: 0.0419 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0441\n",
      "Epoch 12: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0442 - val_loss: 0.0391 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0436\n",
      "Epoch 13: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0436 - val_loss: 0.0371 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0437\n",
      "Epoch 14: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0437 - val_loss: 0.0382 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0438\n",
      "Epoch 15: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0439 - val_loss: 0.0391 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0435\n",
      "Epoch 16: val_loss did not improve from 0.03709\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0435 - val_loss: 0.0379 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0434\n",
      "Epoch 17: val_loss improved from 0.03709 to 0.03614, saving model to datasets\\UK\\aurn_cams\\models\\bi_gru.h5\n",
      "146/146 [==============================] - 3s 17ms/step - loss: 0.0434 - val_loss: 0.0361 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0436\n",
      "Epoch 18: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0436 - val_loss: 0.0374 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.0432\n",
      "Epoch 19: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0433 - val_loss: 0.0381 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0435\n",
      "Epoch 20: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0435 - val_loss: 0.0399 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0438\n",
      "Epoch 21: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0437 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0423\n",
      "Epoch 22: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0424 - val_loss: 0.0385 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 23: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0425 - val_loss: 0.0409 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0428\n",
      "Epoch 24: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0428 - val_loss: 0.0379 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0428\n",
      "Epoch 25: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0428 - val_loss: 0.0371 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.0428\n",
      "Epoch 26: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0428 - val_loss: 0.0381 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0427\n",
      "Epoch 27: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 3s 18ms/step - loss: 0.0427 - val_loss: 0.0392 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0419\n",
      "Epoch 28: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 3s 20ms/step - loss: 0.0419 - val_loss: 0.0367 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 29: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0412 - val_loss: 0.0374 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "142/146 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Epoch 30: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0411 - val_loss: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Epoch 31: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 16ms/step - loss: 0.0411 - val_loss: 0.0371 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "143/146 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Epoch 32: val_loss did not improve from 0.03614\n",
      "146/146 [==============================] - 2s 17ms/step - loss: 0.0410 - val_loss: 0.0367 - lr: 1.0000e-04\n",
      "bi_gru model saved!\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_models(models_folder, sequence_length, X_train, y_train, X_test, y_test, epochs=500):\n",
    "    models = {\n",
    "        \"lstm\": lstm_model(sequence_length, X_train.shape[2]),\n",
    "        \"bi_lstm\": bi_lstm_model(sequence_length, X_train.shape[2]),\n",
    "        \"gru\": gru_model(sequence_length, X_train.shape[2]),\n",
    "        \"bi_gru\": bi_gru_model(sequence_length, X_train.shape[2]),\n",
    "    }\n",
    "    \n",
    "    os.makedirs(Path(models_folder), exist_ok=True)\n",
    "    huber = tf.keras.losses.Huber(delta=0.0125)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        model.compile(optimizer=\"adam\", loss=huber)\n",
    "        \n",
    "        callbacks_list = [\n",
    "            callbacks.ModelCheckpoint(\n",
    "                filepath=Path(models_folder) / f\"{model_name}.h5\",\n",
    "                monitor=\"val_loss\",\n",
    "                verbose=1,\n",
    "                save_weights_only=False,\n",
    "                save_best_only=True,\n",
    "            ),\n",
    "            callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.1,\n",
    "                patience=10,\n",
    "                min_lr=0.0\n",
    "            ),\n",
    "            callbacks.EarlyStopping(\n",
    "                monitor='val_loss',        \n",
    "                patience=15,                \n",
    "                min_delta=0.001,           \n",
    "                restore_best_weights=True   \n",
    "            )            \n",
    "        ]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=32, ## TODO: Change\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "        \n",
    "        model.save(Path(models_folder) / f\"{model_name}.h5\")\n",
    "        print(f\"{model_name} model saved!\")\n",
    "\n",
    "# train the model\n",
    "train_and_save_models(\n",
    "    models_folder, \n",
    "    sequence_length, \n",
    "    X_train, y_train, \n",
    "    X_test, y_test,\n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656be49-2160-445e-b306-0e255a9ec9b3",
   "metadata": {},
   "source": [
    "### 5. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099dcc2-f183-4269-b708-b5e9371d5449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lstm model...\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Evaluating bi_lstm model...\n",
      "24/24 [==============================] - 4s 25ms/step\n",
      "Evaluating gru model...\n",
      "24/24 [==============================] - 0s 4ms/step\n",
      "Evaluating bi_gru model...\n",
      "24/24 [==============================] - 2s 21ms/step\n",
      "Evaluation results saved in directory: datasets\\UK\\aurn_cams\\results\\bi_gru\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_models(models_folder, X_test, y_test, X_test_meta, output_dir, site_column=\"SiteNumber\"):\n",
    "       \n",
    "    models = {model_name: tf.keras.models.load_model(Path(models_folder) / f\"{model_name}.h5\") \n",
    "              for model_name in [\"lstm\", \"bi_lstm\", \"gru\", \"bi_gru\"]}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        output_folder = output_dir/Path(model_name)\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Evaluating {model_name} model...\")\n",
    "        results = []\n",
    "        predictions_list = []  # Store predictions for CSV\n",
    "    \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.round(y_pred).astype(int) # since cases are count we round to nearest int\n",
    "        \n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"MSLE\": mean_squared_log_error(y_test, y_pred),\n",
    "            \"R-squared\": r2_score(y_test, y_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "        pd.DataFrame(results).to_csv(Path(output_folder) / f\"{model_name}_metrics.csv\", index=False)\n",
    "        \n",
    "\n",
    "        if isinstance(X_test_meta, np.ndarray):\n",
    "            expected_columns=[\"Start_Time\", \"End_Time\", \"SiteNumber\"]\n",
    "            X_test_meta = pd.DataFrame(X_test_meta, columns=expected_columns)\n",
    "        \n",
    "        # Collect data for CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"Time\": X_test_meta[\"End_Time\"].values.flatten(),\n",
    "            \"SiteNumber\": X_test_meta[\"SiteNumber\"].values.flatten(),\n",
    "            \"True_Value\": y_test.flatten(),\n",
    "            \"Predicted_Value\": y_pred.flatten()\n",
    "        })\n",
    "        predictions_list.append(predictions_df)\n",
    "        \n",
    "\n",
    "        # Save all predictions for this model\n",
    "        all_predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "        \n",
    "        # Apply function to each row to get the SiteName, Longitude and Latitudes\n",
    "        all_predictions_df[['SiteName', 'Longitude', 'Latitude']] = all_predictions_df['SiteNumber'].apply(\n",
    "            lambda x: pd.Series(get_site_info(x, stations_file))\n",
    "        )\n",
    "\n",
    "        all_predictions_df.to_csv(output_folder / f\"{model_name}_predictions.csv\", index=False)\n",
    "\n",
    "    print(f\"Evaluation results saved in directory: {output_folder}\")\n",
    "\n",
    "\n",
    "output_dir=Path(dataset_folder)/Path(model_dataset)/Path('results')\n",
    "evaluate_all_models(\n",
    "    models_folder, \n",
    "    X_test, y_test, X_test_meta, \n",
    "    output_dir=output_dir,\n",
    "    site_column=\"SiteNumber\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1bbe7-d1ef-4c73-9906-5117f56d9d89",
   "metadata": {},
   "source": [
    "### 6. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6154832-1c1b-42f7-b30a-4cfd2555739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "from keras.models import Model\n",
    "\n",
    "def ensemble_model(models):\n",
    "    # Get the inputs and outputs of the individual models\n",
    "    inputs = [model.input for model in models]\n",
    "    predictions = [model.output for model in models]\n",
    "    \n",
    "    # Average the predictions\n",
    "    ensemble_output = Average()(predictions)\n",
    "    \n",
    "    # Create the ensemble model\n",
    "    ensemble = Model(inputs=inputs, outputs=ensemble_output)\n",
    "    \n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ad99732-55e3-4b8e-8fb1-24d4edaecf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0985   \n",
      "Epoch 1: val_loss improved from inf to 0.05236, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 34s 90ms/step - loss: 0.0984 - val_loss: 0.0524 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0521\n",
      "Epoch 2: val_loss improved from 0.05236 to 0.04136, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 7s 48ms/step - loss: 0.0521 - val_loss: 0.0414 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 3: val_loss improved from 0.04136 to 0.03905, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0464 - val_loss: 0.0390 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0450\n",
      "Epoch 4: val_loss improved from 0.03905 to 0.03778, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 7s 51ms/step - loss: 0.0450 - val_loss: 0.0378 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0444\n",
      "Epoch 5: val_loss improved from 0.03778 to 0.03678, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 8s 51ms/step - loss: 0.0444 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0437\n",
      "Epoch 6: val_loss did not improve from 0.03678\n",
      "146/146 [==============================] - 9s 65ms/step - loss: 0.0437 - val_loss: 0.0374 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0436\n",
      "Epoch 7: val_loss did not improve from 0.03678\n",
      "146/146 [==============================] - 7s 48ms/step - loss: 0.0436 - val_loss: 0.0380 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0434\n",
      "Epoch 8: val_loss did not improve from 0.03678\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0434 - val_loss: 0.0376 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0435\n",
      "Epoch 9: val_loss did not improve from 0.03678\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0435 - val_loss: 0.0387 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0429\n",
      "Epoch 10: val_loss improved from 0.03678 to 0.03672, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0429 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0428\n",
      "Epoch 11: val_loss did not improve from 0.03672\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0428 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 12: val_loss improved from 0.03672 to 0.03592, saving model to datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0425 - val_loss: 0.0359 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0423\n",
      "Epoch 13: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0423 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0422\n",
      "Epoch 14: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0423 - val_loss: 0.0383 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 15: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 47ms/step - loss: 0.0425 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0424\n",
      "Epoch 16: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 47ms/step - loss: 0.0424 - val_loss: 0.0385 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0421\n",
      "Epoch 17: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0421 - val_loss: 0.0405 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0420\n",
      "Epoch 18: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0420 - val_loss: 0.0365 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0420\n",
      "Epoch 19: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 49ms/step - loss: 0.0420 - val_loss: 0.0372 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0419\n",
      "Epoch 20: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0418 - val_loss: 0.0403 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0419\n",
      "Epoch 21: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0419 - val_loss: 0.0374 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0419\n",
      "Epoch 22: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0419 - val_loss: 0.0367 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Epoch 23: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 45ms/step - loss: 0.0410 - val_loss: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 24/500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0408\n",
      "Epoch 24: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0408 - val_loss: 0.0367 - lr: 1.0000e-04\n",
      "Epoch 25/500\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 25: val_loss did not improve from 0.03592\n",
      "146/146 [==============================] - 7s 46ms/step - loss: 0.0407 - val_loss: 0.0366 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "def train_ensemble_model(X_train, y_train, X_test, y_test, sequence_length, models_folder, model_fns, epochs=500):\n",
    "    # Create each model dynamically using the passed functions\n",
    "    models = [model_fn(sequence_length, X_train.shape[2]) for model_fn in model_fns]\n",
    "\n",
    "    # Create the ensemble model\n",
    "    model = ensemble_model(models)  # Assuming ensemble_model function takes a list of models\n",
    "\n",
    "    # Define the loss function and compile the model\n",
    "    huber = tf.keras.losses.Huber(delta=0.0125)\n",
    "    model.compile(optimizer='adam', loss=huber)\n",
    "\n",
    "    # Create the model directory if it doesn't exist\n",
    "    os.makedirs(Path(models_folder), exist_ok=True)\n",
    "    path_checkpoint = Path(models_folder) / Path(\"ensemble.h5\")\n",
    "\n",
    "    # Define the callbacks\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, \n",
    "        patience=10, \n",
    "        min_lr=0.0\n",
    "    )\n",
    "\n",
    "    modelckpt_callback = callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=path_checkpoint,\n",
    "        verbose=1,\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',        \n",
    "        patience=15,                \n",
    "        min_delta=0.001,           \n",
    "        restore_best_weights=True   \n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train, X_train, X_train, X_train], y_train, \n",
    "        validation_data=([X_test, X_test, X_test, X_test], y_test), \n",
    "        epochs=epochs, \n",
    "        batch_size=32, \n",
    "        callbacks=[modelckpt_callback, reduce_lr, early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "model_fns = [lstm_model, bi_lstm_model, gru_model, bi_gru_model]  # Pass model creation functions as a list\n",
    "history = train_ensemble_model(\n",
    "    X_train, y_train, \n",
    "    X_test, y_test, \n",
    "    sequence_length, \n",
    "    models_folder, \n",
    "    model_fns, \n",
    "    epochs=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03406d7e-f102-42ac-996d-3f853056dfdb",
   "metadata": {},
   "source": [
    "### 7. Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc92cb-b86e-4df1-acfe-a8649ddc7acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\UK\\aurn_cams\\models\\ensemble.h5\n",
      "Evaluating ensemble model...\n",
      "24/24 [==============================] - 6s 38ms/step\n",
      "Evaluation results saved in directory: datasets\\UK\\aurn_cams\\results\\ensemble\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ensemble(models_folder, X_test, y_test, X_test_meta, output_dir, model_name, site_column=\"SiteNumber\"):\n",
    "    print(Path(models_folder) / f\"{model_name}.h5\")\n",
    "    model = tf.keras.models.load_model(Path(models_folder) / f\"{model_name}.h5\") \n",
    "    \n",
    "    output_folder = output_dir/Path(model_name)\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    print(f\"Evaluating {model_name} model...\")\n",
    "    results = []\n",
    "    predictions_list = []\n",
    "        \n",
    "    y_pred = model.predict([X_test, X_test, X_test, X_test])\n",
    "    \n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"MSLE\": mean_squared_log_error(y_test, y_pred),\n",
    "        \"R-squared\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "    results.append(metrics)\n",
    "    pd.DataFrame(results).to_csv(Path(output_folder) / f\"{model_name}_metrics.csv\", index=False)\n",
    "\n",
    "    if isinstance(X_test_meta, np.ndarray):\n",
    "            expected_columns=[\"Start_Time\", \"End_Time\", \"SiteNumber\"]\n",
    "            X_test_meta = pd.DataFrame(X_test_meta, columns=expected_columns)\n",
    "    \n",
    "    # Collect data for CSV\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"Time\": X_test_meta[\"End_Time\"].values.flatten(),\n",
    "        \"SiteNumber\": X_test_meta[\"SiteNumber\"].values.flatten(),\n",
    "        \"True_Value\": y_test.flatten(),\n",
    "        \"Predicted_Value\": y_pred.flatten()\n",
    "    })\n",
    "    predictions_list.append(predictions_df)\n",
    "        \n",
    "    # Save all predictions for this model\n",
    "    all_predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "    \n",
    "    # Apply function to each row to get the SiteName, Longitude and Latitudes\n",
    "    all_predictions_df[['SiteName', 'Longitude', 'Latitude']] = all_predictions_df['SiteNumber'].apply(\n",
    "        lambda x: pd.Series(get_site_info(x, stations_file))\n",
    "    )\n",
    "\n",
    "    all_predictions_df.to_csv(output_folder / f\"{model_name}_predictions.csv\", index=False)\n",
    "\n",
    "    print(f\"Evaluation results saved in directory: {output_folder}\")\n",
    "\n",
    "\n",
    "output_dir=Path(dataset_folder)/Path(model_dataset)/Path('results')\n",
    "evaluate_ensemble(\n",
    "    models_folder, \n",
    "    X_test, y_test, X_test_meta,\n",
    "    output_dir, \n",
    "    model_name=\"ensemble\",\n",
    "    site_column=\"SiteNumber\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
